{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import zipfile\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = zipfile.ZipFile('train.json.zip','r')\n",
    "train_data = pd.read_json(train_data.read('train.json'))\n",
    "test_data = zipfile.ZipFile('test.json.zip','r')\n",
    "test_data = pd.read_json(test_data.read('test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {c for cs in train_data['ingredients'] for c in cs}\n",
    "\n",
    "test_categories = {c for cs in test_data['ingredients'] for c in cs}\n",
    "\n",
    "df = pd.DataFrame(dtype='u1', index=range(len(train_data['ingredients'])), columns=categories)\n",
    "df.fillna(0, inplace=True)\n",
    "df['cuisine'] = train_data['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_data['ingredients'].iteritems():\n",
    "    df.loc[i, row] = 1\n",
    "\n",
    "dd = df.groupby(['cuisine']).sum()\n",
    "\n",
    "col_sums = dd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(learn, test=None):\n",
    "    cols = learn.drop(['cuisine'], axis=1).columns\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(learn[cols],\n",
    "                                                            learn['cuisine'],\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=learn['cuisine'])\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = learn[cols], test[cols], learn['cuisine'], None\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    logit.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = logit.predict(test_x)\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    return logit.predict(x_train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "source": [
    "Unadulterated training data performance:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array(['mexican', 'irish', 'mexican', ..., 'mexican', 'mexican',\n",
       "        'southern_us'], dtype=object),\n",
       " 0.7731295253419147)"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "learn(df)"
   ]
  },
  {
   "source": [
    "Can we do better than that?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        sum\n0       9.0\n1      11.0\n2      12.0\n3       4.0\n4      20.0\n...     ...\n39769  12.0\n39770   7.0\n39771  12.0\n39772  21.0\n39773  12.0\n\n[39774 rows x 1 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38266"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "recipe_complexity = pd.DataFrame({'sum': df[dd.columns].sum(axis=1)})\n",
    "\n",
    "print(recipe_complexity)\n",
    "\n",
    "recipe_complexity['cuisine'] = df['cuisine']\n",
    "\n",
    "len(recipe_complexity[recipe_complexity['sum'] < 20])"
   ]
  },
  {
   "source": [
    "Is there a difference in how short recipes are distributed?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6194503171247357"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "learn(df[recipe_complexity['sum'] < 6])"
   ]
  },
  {
   "source": [
    "I wonder if using ingredients as-is is a good idea. Maybe \"pork chops\" are the same, no matter the brand?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "words = {w for ws in train_data['ingredients'] for w in ws for w in w.split()}\n",
    "len(words) # much fewer traits for sure"
   ]
  },
  {
   "source": [
    "How about ingredients that seem to be generic?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "worthy_cols = {}\n",
    "cuisine_groups = {}\n",
    "for c in col_sums.index:\n",
    "    cc = dd[c][dd[c] > 0]\n",
    "    if len(cc) < 2:\n",
    "        if len(cc) == 1 and cc[0] > 5:\n",
    "            worthy_cols[c] = 0, 0\n",
    "        else:\n",
    "            cuisine_groups[c] = ';'.join(cc.index)\n",
    "        continue\n",
    "    s, p = stats.chisquare(cc)\n",
    "    \n",
    "    if p > 0.95: # ignore ingredients that are distributed too uniformly across cuisines\n",
    "        cuisine_groups[c] = ';'.join(cc.index)\n",
    "        continue\n",
    "    worthy_cols[c] = s, p\n",
    "worthy_cols = pd.DataFrame(worthy_cols)\n",
    "\n",
    "worthy_cols['cuisine'] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "len(dd[cuisine_groups.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14           italian\n",
       "99           italian\n",
       "231          italian\n",
       "238          italian\n",
       "252      southern_us\n",
       "            ...     \n",
       "39333        italian\n",
       "39398    southern_us\n",
       "39525        italian\n",
       "39625        mexican\n",
       "39682    southern_us\n",
       "Name: cuisine, Length: 745, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "test_x = df[cuisine_groups.keys()]\n",
    "cuisine_groups['cuisine'] = ''\n",
    "test_groups = learn(df[cuisine_groups.keys()], test_x)\n",
    "\n",
    "test_x = df[worthy_cols.columns].drop('cuisine', axis=1)\n",
    "test_worthy = learn(df[worthy_cols.columns], test_x)\n",
    "\n",
    "df['cuisine'][np.logical_and(test_worthy != df['cuisine'], test_groups == df['cuisine'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "jamaican         87\n",
       "korean           96\n",
       "moroccan        101\n",
       "brazilian       115\n",
       "filipino        146\n",
       "russian         160\n",
       "indian          162\n",
       "thai            170\n",
       "vietnamese      195\n",
       "irish           207\n",
       "chinese         213\n",
       "greek           227\n",
       "british         248\n",
       "japanese        269\n",
       "cajun_creole    289\n",
       "mexican         307\n",
       "spanish         354\n",
       "italian         502\n",
       "southern_us     508\n",
       "french          609\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "vietnamese        4\n",
       "moroccan          4\n",
       "thai              9\n",
       "korean           10\n",
       "greek            16\n",
       "indian           20\n",
       "japanese         21\n",
       "jamaican         23\n",
       "brazilian        23\n",
       "chinese          23\n",
       "filipino         27\n",
       "russian          29\n",
       "spanish          35\n",
       "irish            68\n",
       "british          88\n",
       "italian          98\n",
       "mexican         101\n",
       "french          122\n",
       "cajun_creole    157\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "df[np.logical_and(test_worthy == 'southern_us', test_worthy != df['cuisine'])].groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "brazilian        467\n",
       "russian          489\n",
       "jamaican         526\n",
       "irish            667\n",
       "filipino         755\n",
       "british          804\n",
       "moroccan         821\n",
       "vietnamese       825\n",
       "korean           830\n",
       "spanish          989\n",
       "greek           1175\n",
       "japanese        1423\n",
       "thai            1539\n",
       "cajun_creole    1546\n",
       "french          2646\n",
       "chinese         2673\n",
       "indian          3003\n",
       "southern_us     4320\n",
       "mexican         6438\n",
       "italian         7838\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df.groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "mexican         0.047686\n",
       "indian          0.053946\n",
       "italian         0.064047\n",
       "chinese         0.079686\n",
       "thai            0.110461\n",
       "korean          0.115663\n",
       "southern_us     0.117593\n",
       "moroccan        0.123021\n",
       "jamaican        0.165399\n",
       "cajun_creole    0.186934\n",
       "japanese        0.189037\n",
       "greek           0.193191\n",
       "filipino        0.193377\n",
       "french          0.230159\n",
       "vietnamese      0.236364\n",
       "brazilian       0.246253\n",
       "british         0.308458\n",
       "irish           0.310345\n",
       "russian         0.327198\n",
       "spanish         0.357937\n",
       "Name: stew, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "(df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew'] / df.groupby('cuisine').count()['stew'].sort_values()).sort_values()"
   ]
  },
  {
   "source": [
    "ok, we know french, southern_us, mexican, italian get confused a lot, especially french (large error) and italian (large population).\n",
    "\n",
    "Let's slice them off into separate classifiers.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_french(train, test=None):\n",
    "    cuisine = train['cuisine']\n",
    "    train = train.drop('cuisine', axis = 1)\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(train,\n",
    "                                                            cuisine,\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = train, test, cuisine, None\n",
    "\n",
    "    y_train_generic = y_train.copy()\n",
    "    select = np.logical_or(y_train == 'french', y_train == 'italian'), y_train == 'southern_us' #np.logical_or(y_train == 'southern_us', y_train == 'mexican')\n",
    "    y_train_generic[np.logical_or(*select)] = 'french'\n",
    "    #y_train_generic[select[0]] = 'french'\n",
    "\n",
    "    logit_generic = LogisticRegression()\n",
    "    logit_generic.fit(x_train, y_train_generic)\n",
    "\n",
    "    print('Done Generic')\n",
    "\n",
    "    logit_french = LogisticRegression()\n",
    "    logit_french.fit(x_train[y_train_generic == 'french'], y_train[y_train_generic == 'french'])\n",
    "\n",
    "    print('Done French')\n",
    "\n",
    "    def predict(test_x):\n",
    "        y_pred = logit_generic.predict(test_x)\n",
    "        y_pred[y_pred == 'french'] = logit_french.predict(test_x[y_pred == 'french'])\n",
    "        return y_pred\n",
    "\n",
    "    y_pred = predict(test_x)\n",
    "\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "\n",
    "    return predict(train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_not_french(train, test=None):\n",
    "    cuisine = train['cuisine']\n",
    "    train = train.drop('cuisine', axis = 1)\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(train,\n",
    "                                                            cuisine,\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = train, test, cuisine, None\n",
    "\n",
    "    y_train_french = y_train.copy()\n",
    "    select = np.logical_and(y_train != 'french', y_train != 'mexican'), np.logical_and(y_train != 'southern_us', y_train != 'italian')\n",
    "    y_train_french[np.logical_and(*select)] = 'not_french'\n",
    "\n",
    "    logit_french = LogisticRegression()\n",
    "    logit_french.fit(x_train, y_train_french)\n",
    "\n",
    "    logit_generic = LogisticRegression()\n",
    "    logit_generic.fit(x_train[y_train_french == 'not_french'], y_train[y_train_french == 'not_french'])\n",
    "\n",
    "    def predict(test_x):\n",
    "        y_pred = logit_french.predict(test_x)\n",
    "        y_pred[y_pred == 'not_french'] = logit_generic.predict(test_x[y_pred == 'not_french'])\n",
    "        return y_pred\n",
    "\n",
    "    y_pred = predict(test_x)\n",
    "\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    return predict(train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Done Generic\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Done French\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7698109412711183"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "test_french, accuracy = learn_french(df[worthy_cols.columns])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "italian           1\n",
       "brazilian         7\n",
       "jamaican         15\n",
       "korean           21\n",
       "vietnamese       21\n",
       "japanese         24\n",
       "filipino         26\n",
       "moroccan         28\n",
       "russian          31\n",
       "chinese          36\n",
       "spanish          36\n",
       "greek            45\n",
       "indian           45\n",
       "irish            49\n",
       "cajun_creole     49\n",
       "thai             53\n",
       "mexican          62\n",
       "british          64\n",
       "french           95\n",
       "southern_us     133\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "(df[test_french != df['cuisine']].groupby('cuisine').count()['stew'] - df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       enokitake  frozen corn kernels  part-skim mozzarella  soda  \\\n",
       "0            0.0                  0.0                   0.0   0.0   \n",
       "1            0.0                  0.0                   0.0   0.0   \n",
       "2            0.0                  0.0                   0.0   0.0   \n",
       "3            0.0                  0.0                   0.0   0.0   \n",
       "4            0.0                  0.0                   0.0   0.0   \n",
       "...          ...                  ...                   ...   ...   \n",
       "39769        0.0                  0.0                   0.0   0.0   \n",
       "39770        0.0                  0.0                   0.0   0.0   \n",
       "39771        0.0                  0.0                   0.0   0.0   \n",
       "39772        0.0                  0.0                   0.0   0.0   \n",
       "39773        0.0                  0.0                   0.0   0.0   \n",
       "\n",
       "       california chile  condensed cream of celery soup  white bread flour  \\\n",
       "0                   0.0                             0.0                0.0   \n",
       "1                   0.0                             0.0                0.0   \n",
       "2                   0.0                             0.0                0.0   \n",
       "3                   0.0                             0.0                0.0   \n",
       "4                   0.0                             0.0                0.0   \n",
       "...                 ...                             ...                ...   \n",
       "39769               0.0                             0.0                0.0   \n",
       "39770               0.0                             0.0                0.0   \n",
       "39771               0.0                             0.0                0.0   \n",
       "39772               0.0                             0.0                0.0   \n",
       "39773               0.0                             0.0                0.0   \n",
       "\n",
       "       chile paste  basil  extra large shrimp  ...  ale  fried eggs  \\\n",
       "0              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "1              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "2              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "3              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "4              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "...            ...    ...                 ...  ...  ...         ...   \n",
       "39769          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39770          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39771          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39772          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39773          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "\n",
       "       asparagus tips  sofrito  calvados  field peas  fenugreek seeds  \\\n",
       "0                 0.0      0.0       0.0         0.0              0.0   \n",
       "1                 0.0      0.0       0.0         0.0              0.0   \n",
       "2                 0.0      0.0       0.0         0.0              0.0   \n",
       "3                 0.0      0.0       0.0         0.0              0.0   \n",
       "4                 0.0      0.0       0.0         0.0              0.0   \n",
       "...               ...      ...       ...         ...              ...   \n",
       "39769             0.0      0.0       0.0         0.0              0.0   \n",
       "39770             0.0      0.0       0.0         0.0              0.0   \n",
       "39771             0.0      0.0       0.0         0.0              0.0   \n",
       "39772             0.0      0.0       0.0         0.0              0.0   \n",
       "39773             0.0      0.0       0.0         0.0              0.0   \n",
       "\n",
       "       short-grain rice  glace cherries      cuisine  \n",
       "0                   0.0             0.0        greek  \n",
       "1                   0.0             0.0  southern_us  \n",
       "2                   0.0             0.0     filipino  \n",
       "3                   0.0             0.0       indian  \n",
       "4                   0.0             0.0       indian  \n",
       "...                 ...             ...          ...  \n",
       "39769               0.0             0.0        irish  \n",
       "39770               0.0             0.0      italian  \n",
       "39771               0.0             0.0        irish  \n",
       "39772               0.0             0.0      chinese  \n",
       "39773               0.0             0.0      mexican  \n",
       "\n",
       "[39774 rows x 3336 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>enokitake</th>\n      <th>frozen corn kernels</th>\n      <th>part-skim mozzarella</th>\n      <th>soda</th>\n      <th>california chile</th>\n      <th>condensed cream of celery soup</th>\n      <th>white bread flour</th>\n      <th>chile paste</th>\n      <th>basil</th>\n      <th>extra large shrimp</th>\n      <th>...</th>\n      <th>ale</th>\n      <th>fried eggs</th>\n      <th>asparagus tips</th>\n      <th>sofrito</th>\n      <th>calvados</th>\n      <th>field peas</th>\n      <th>fenugreek seeds</th>\n      <th>short-grain rice</th>\n      <th>glace cherries</th>\n      <th>cuisine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>greek</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>southern_us</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>filipino</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>indian</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>indian</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39769</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>irish</td>\n    </tr>\n    <tr>\n      <th>39770</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>italian</td>\n    </tr>\n    <tr>\n      <th>39771</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>irish</td>\n    </tr>\n    <tr>\n      <th>39772</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>chinese</td>\n    </tr>\n    <tr>\n      <th>39773</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>mexican</td>\n    </tr>\n  </tbody>\n</table>\n<p>39774 rows Ã— 3336 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df[worthy_cols.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayes(learn, test=None):\n",
    "    cols = learn.drop(['cuisine'], axis=1).columns\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(learn[cols],\n",
    "                                                            learn['cuisine'],\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = learn[cols], test[cols], learn['cuisine'], None\n",
    "\n",
    "    var = x_train.std()\n",
    "    var = var[var < 1e-15]\n",
    "    if len(var):\n",
    "        x_train = x_train.drop(var.index, axis=1)\n",
    "        test_x = test_x.drop(var.index, axis=1)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = nb.predict(test_x)\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    return nb.predict(x_train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7352172164119066"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "test_bayes, accuracy = learn_bayes(df)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(dd[worthy_cols.columns].drop('cuisine', axis=1))"
   ]
  },
  {
   "source": [
    "Can cross-correlation help eliminate \"noise\"? (reduce overfitting)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine         french   italian  southern_us  cajun_creole\n",
       "cuisine                                                    \n",
       "french        1.000000  0.822349     0.849405      0.697334\n",
       "italian       0.822349  1.000000     0.684260      0.712391\n",
       "southern_us   0.849405  0.684260     1.000000      0.710662\n",
       "cajun_creole  0.697334  0.712391     0.710662      1.000000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>cuisine</th>\n      <th>french</th>\n      <th>italian</th>\n      <th>southern_us</th>\n      <th>cajun_creole</th>\n    </tr>\n    <tr>\n      <th>cuisine</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>french</th>\n      <td>1.000000</td>\n      <td>0.822349</td>\n      <td>0.849405</td>\n      <td>0.697334</td>\n    </tr>\n    <tr>\n      <th>italian</th>\n      <td>0.822349</td>\n      <td>1.000000</td>\n      <td>0.684260</td>\n      <td>0.712391</td>\n    </tr>\n    <tr>\n      <th>southern_us</th>\n      <td>0.849405</td>\n      <td>0.684260</td>\n      <td>1.000000</td>\n      <td>0.710662</td>\n    </tr>\n    <tr>\n      <th>cajun_creole</th>\n      <td>0.697334</td>\n      <td>0.712391</td>\n      <td>0.710662</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "corr = dd[col_sums.index].transpose()[['french', 'italian', 'southern_us', 'cajun_creole']].corr()\n",
    "#for c in col_sums.index:\n",
    "#    corr.loc[c, c] = 0\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorr_cols = set(col_sums.index)\n",
    "for c in col_sums.index:\n",
    "    excess = corr[c] > 0.9\n",
    "    for c in excess.index:\n",
    "        if excess[c] and c in uncorr_cols:\n",
    "            uncorr_cols.remove(c)\n",
    "uncorr_cols.add('cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f80cd887e0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muncorr_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# oops, not a good idea after all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2cc59ec2fcbd>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(learn, test)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#gnb = CategoricalNB()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "learn(df[uncorr_cols]) # oops, not a good idea after all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8331321969125559"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipeline(train_x, train_y):\n",
    "    pipe = Pipeline([('count', CountVectorizer(tokenizer=lambda x: x, lowercase=False)),\n",
    "                     ('freq', TfidfTransformer()),\n",
    "                     ('logit', LogisticRegression(solver='newton-cg'))])\n",
    "    pipe.fit(train_x, train_y)\n",
    "    return pipe\n",
    "\n",
    "pipe = pipeline(train_data['ingredients'], train_data['cuisine'])\n",
    "pipe.score(train_data['ingredients'], train_data['cuisine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan(train_x):\n",
    "    pipe = Pipeline([('count', CountVectorizer(tokenizer=lambda x: x, lowercase=False)),\n",
    "                     ('cosine_distance', FunctionTransformer(cosine_distances)),\n",
    "                     ('dbscan', DBSCAN(eps=0.3, min_samples=2))])\n",
    "    return pipe.fit(train_x)\n",
    "\n",
    "# dbscan(train_data['ingredients']) - this is insane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}