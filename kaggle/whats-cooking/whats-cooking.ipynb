{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import zipfile\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = zipfile.ZipFile('train.json.zip','r')\n",
    "train_data = pd.read_json(train_data.read('train.json'))\n",
    "test_data = zipfile.ZipFile('test.json.zip','r')\n",
    "test_data = pd.read_json(test_data.read('test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {c for cs in train_data['ingredients'] for c in cs}\n",
    "\n",
    "test_categories = {c for cs in test_data['ingredients'] for c in cs}\n",
    "\n",
    "df = pd.DataFrame(dtype='u1', index=range(len(train_data['ingredients'])), columns=categories)\n",
    "df.fillna(0, inplace=True)\n",
    "df['cuisine'] = train_data['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_data['ingredients'].iteritems():\n",
    "    df.loc[i, row] = 1\n",
    "\n",
    "dd = df.groupby(['cuisine']).sum()\n",
    "\n",
    "col_sums = dd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(learn, test=None):\n",
    "    cols = learn.drop(['cuisine'], axis=1).columns #sum().index\n",
    "#    cols = cols[cols >= 5].index\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(learn[cols],\n",
    "                                                            learn['cuisine'],\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=learn['cuisine'])\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = learn[cols], test[cols], learn['cuisine'], None\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    logit.fit(x_train, y_train)\n",
    "\n",
    "    #gnb = CategoricalNB()\n",
    "    #gnb.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = logit.predict(test_x)#, gnb.predict(test_x)\n",
    "    #y_pred = gnb.predict(test_x)\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    return y_pred, len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "source": [
    "Unadulterated training data performance:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7731295253419147"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "learn(df)"
   ]
  },
  {
   "source": [
    "Can we do better than that?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        sum\n0       9.0\n1      11.0\n2      12.0\n3       4.0\n4      20.0\n...     ...\n39769  12.0\n39770   7.0\n39771  12.0\n39772  21.0\n39773  12.0\n\n[39774 rows x 1 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38266"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "recipe_complexity = pd.DataFrame({'sum': df[dd.columns].sum(axis=1)})\n",
    "\n",
    "print(recipe_complexity)\n",
    "\n",
    "recipe_complexity['cuisine'] = df['cuisine']\n",
    "\n",
    "len(recipe_complexity[recipe_complexity['sum'] < 20])"
   ]
  },
  {
   "source": [
    "Is there a difference in how short recipes are distributed?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6194503171247357"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "learn(df[recipe_complexity['sum'] < 6])"
   ]
  },
  {
   "source": [
    "I wonder if using ingredients as-is is a good idea. Maybe \"pork chops\" are the same, no matter the brand?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "words = {w for ws in train_data['ingredients'] for w in ws for w in w.split()}\n",
    "len(words) # much fewer traits for sure"
   ]
  },
  {
   "source": [
    "How about ingredients that seem to be generic?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "worthy_cols = {}\n",
    "cuisine_groups = {}\n",
    "for c in col_sums.index:\n",
    "    cc = dd[c][dd[c] > 0]\n",
    "    if len(cc) < 2:\n",
    "        if len(cc) == 1 and cc[0] > 5:\n",
    "            worthy_cols[c] = 0, 0\n",
    "        else:\n",
    "            cuisine_groups[c] = ';'.join(cc.index)\n",
    "        continue\n",
    "    s, p = stats.chisquare(cc)\n",
    "    \n",
    "    if p > 0.95: # ignore ingredients that are distributed too uniformly across cuisines\n",
    "        cuisine_groups[c] = ';'.join(cc.index)\n",
    "        continue\n",
    "    worthy_cols[c] = s, p\n",
    "worthy_cols = pd.DataFrame(worthy_cols)\n",
    "\n",
    "worthy_cols['cuisine'] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "len(dd[cuisine_groups.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14           italian\n",
       "99           italian\n",
       "231          italian\n",
       "238          italian\n",
       "252      southern_us\n",
       "            ...     \n",
       "39333        italian\n",
       "39398    southern_us\n",
       "39525        italian\n",
       "39625        mexican\n",
       "39682    southern_us\n",
       "Name: cuisine, Length: 745, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "test_x = df[cuisine_groups.keys()]\n",
    "cuisine_groups['cuisine'] = ''\n",
    "test_groups = learn(df[cuisine_groups.keys()], test_x)\n",
    "\n",
    "test_x = df[worthy_cols.columns].drop('cuisine', axis=1)\n",
    "test_worthy = learn(df[worthy_cols.columns], test_x)\n",
    "\n",
    "df['cuisine'][np.logical_and(test_worthy != df['cuisine'], test_groups == df['cuisine'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "jamaican         87\n",
       "korean           96\n",
       "moroccan        101\n",
       "brazilian       115\n",
       "filipino        146\n",
       "russian         160\n",
       "indian          162\n",
       "thai            170\n",
       "vietnamese      195\n",
       "irish           207\n",
       "chinese         213\n",
       "greek           227\n",
       "british         248\n",
       "japanese        269\n",
       "cajun_creole    289\n",
       "mexican         307\n",
       "spanish         354\n",
       "italian         502\n",
       "southern_us     508\n",
       "french          609\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "vietnamese        4\n",
       "moroccan          4\n",
       "thai              9\n",
       "korean           10\n",
       "greek            16\n",
       "indian           20\n",
       "japanese         21\n",
       "jamaican         23\n",
       "brazilian        23\n",
       "chinese          23\n",
       "filipino         27\n",
       "russian          29\n",
       "spanish          35\n",
       "irish            68\n",
       "british          88\n",
       "italian          98\n",
       "mexican         101\n",
       "french          122\n",
       "cajun_creole    157\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "df[np.logical_and(test_worthy == 'southern_us', test_worthy != df['cuisine'])].groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "brazilian        467\n",
       "russian          489\n",
       "jamaican         526\n",
       "irish            667\n",
       "filipino         755\n",
       "british          804\n",
       "moroccan         821\n",
       "vietnamese       825\n",
       "korean           830\n",
       "spanish          989\n",
       "greek           1175\n",
       "japanese        1423\n",
       "thai            1539\n",
       "cajun_creole    1546\n",
       "french          2646\n",
       "chinese         2673\n",
       "indian          3003\n",
       "southern_us     4320\n",
       "mexican         6438\n",
       "italian         7838\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df.groupby('cuisine').count()['stew'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "mexican         0.047686\n",
       "indian          0.053946\n",
       "italian         0.064047\n",
       "chinese         0.079686\n",
       "thai            0.110461\n",
       "korean          0.115663\n",
       "southern_us     0.117593\n",
       "moroccan        0.123021\n",
       "jamaican        0.165399\n",
       "cajun_creole    0.186934\n",
       "japanese        0.189037\n",
       "greek           0.193191\n",
       "filipino        0.193377\n",
       "french          0.230159\n",
       "vietnamese      0.236364\n",
       "brazilian       0.246253\n",
       "british         0.308458\n",
       "irish           0.310345\n",
       "russian         0.327198\n",
       "spanish         0.357937\n",
       "Name: stew, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "(df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew'] / df.groupby('cuisine').count()['stew'].sort_values()).sort_values()"
   ]
  },
  {
   "source": [
    "ok, we know french, southern_us, mexican, italian get confused a lot, especially french (large error) and italian (large population).\n",
    "\n",
    "Let's slice them off into separate classifiers.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_french(train, test=None):\n",
    "    cuisine = train['cuisine']\n",
    "    train = train.drop('cuisine', axis = 1)\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(train,\n",
    "                                                            cuisine,\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = train, test, cuisine, None\n",
    "\n",
    "    y_train_generic = y_train.copy()\n",
    "    select = np.logical_or(y_train == 'french', y_train == 'mexican'), np.logical_or(y_train == 'southern_us', y_train == 'italian')\n",
    "    y_train_generic[np.logical_or(*select)] = 'french'\n",
    "\n",
    "    logit_generic = LogisticRegression()\n",
    "    logit_generic.fit(x_train, y_train_generic)\n",
    "\n",
    "    logit_french = LogisticRegression()\n",
    "    logit_french.fit(x_train[y_train_generic == 'french'], y_train[y_train_generic == 'french'])\n",
    "\n",
    "    def predict(test_x):\n",
    "        y_pred = logit_generic.predict(test_x)\n",
    "        y_pred[y_pred == 'french'] = logit_french.predict(test_x[y_pred == 'french'])\n",
    "        return y_pred\n",
    "\n",
    "    y_pred = predict(test_x)\n",
    "\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "\n",
    "    return predict(train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_not_french(train, test=None):\n",
    "    cuisine = train['cuisine']\n",
    "    train = train.drop('cuisine', axis = 1)\n",
    "\n",
    "    if test is None:\n",
    "        x_train, test_x, y_train, test_y = train_test_split(train,\n",
    "                                                            cuisine,\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        x_train, test_x, y_train, test_y = train, test, cuisine, None\n",
    "\n",
    "    y_train_french = y_train.copy()\n",
    "    select = np.logical_and(y_train != 'french', y_train != 'mexican'), np.logical_and(y_train != 'southern_us', y_train != 'italian')\n",
    "    y_train_french[np.logical_and(*select)] = 'not_french'\n",
    "\n",
    "    logit_french = LogisticRegression()\n",
    "    logit_french.fit(x_train, y_train_french)\n",
    "\n",
    "    logit_generic = LogisticRegression()\n",
    "    logit_generic.fit(x_train[y_train_french == 'not_french'], y_train[y_train_french == 'not_french'])\n",
    "\n",
    "    def predict(test_x):\n",
    "        y_pred = logit_french.predict(test_x)\n",
    "        y_pred[y_pred == 'not_french'] = logit_generic.predict(test_x[y_pred == 'not_french'])\n",
    "        return y_pred\n",
    "\n",
    "    y_pred = predict(test_x)\n",
    "\n",
    "    if test_y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    return predict(train), len(y_pred[y_pred == test_y]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7663917940466614"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "test_french, accuracy = learn_french(df[worthy_cols.columns])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "cuisine\n",
       "italian           2\n",
       "mexican          18\n",
       "southern_us      21\n",
       "french           24\n",
       "vietnamese       26\n",
       "korean           27\n",
       "jamaican         34\n",
       "japanese         38\n",
       "brazilian        38\n",
       "moroccan         46\n",
       "filipino         56\n",
       "chinese          58\n",
       "thai             58\n",
       "russian          59\n",
       "greek            70\n",
       "indian           75\n",
       "irish            78\n",
       "spanish          97\n",
       "british         120\n",
       "cajun_creole    140\n",
       "Name: stew, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "(df[test_french != df['cuisine']].groupby('cuisine').count()['stew'] - df[test_worthy != df['cuisine']].groupby('cuisine').count()['stew']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       enokitake  frozen corn kernels  part-skim mozzarella  soda  \\\n",
       "0            0.0                  0.0                   0.0   0.0   \n",
       "1            0.0                  0.0                   0.0   0.0   \n",
       "2            0.0                  0.0                   0.0   0.0   \n",
       "3            0.0                  0.0                   0.0   0.0   \n",
       "4            0.0                  0.0                   0.0   0.0   \n",
       "...          ...                  ...                   ...   ...   \n",
       "39769        0.0                  0.0                   0.0   0.0   \n",
       "39770        0.0                  0.0                   0.0   0.0   \n",
       "39771        0.0                  0.0                   0.0   0.0   \n",
       "39772        0.0                  0.0                   0.0   0.0   \n",
       "39773        0.0                  0.0                   0.0   0.0   \n",
       "\n",
       "       california chile  condensed cream of celery soup  white bread flour  \\\n",
       "0                   0.0                             0.0                0.0   \n",
       "1                   0.0                             0.0                0.0   \n",
       "2                   0.0                             0.0                0.0   \n",
       "3                   0.0                             0.0                0.0   \n",
       "4                   0.0                             0.0                0.0   \n",
       "...                 ...                             ...                ...   \n",
       "39769               0.0                             0.0                0.0   \n",
       "39770               0.0                             0.0                0.0   \n",
       "39771               0.0                             0.0                0.0   \n",
       "39772               0.0                             0.0                0.0   \n",
       "39773               0.0                             0.0                0.0   \n",
       "\n",
       "       chile paste  basil  extra large shrimp  ...  ale  fried eggs  \\\n",
       "0              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "1              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "2              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "3              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "4              0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "...            ...    ...                 ...  ...  ...         ...   \n",
       "39769          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39770          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39771          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39772          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "39773          0.0    0.0                 0.0  ...  0.0         0.0   \n",
       "\n",
       "       asparagus tips  sofrito  calvados  field peas  fenugreek seeds  \\\n",
       "0                 0.0      0.0       0.0         0.0              0.0   \n",
       "1                 0.0      0.0       0.0         0.0              0.0   \n",
       "2                 0.0      0.0       0.0         0.0              0.0   \n",
       "3                 0.0      0.0       0.0         0.0              0.0   \n",
       "4                 0.0      0.0       0.0         0.0              0.0   \n",
       "...               ...      ...       ...         ...              ...   \n",
       "39769             0.0      0.0       0.0         0.0              0.0   \n",
       "39770             0.0      0.0       0.0         0.0              0.0   \n",
       "39771             0.0      0.0       0.0         0.0              0.0   \n",
       "39772             0.0      0.0       0.0         0.0              0.0   \n",
       "39773             0.0      0.0       0.0         0.0              0.0   \n",
       "\n",
       "       short-grain rice  glace cherries      cuisine  \n",
       "0                   0.0             0.0        greek  \n",
       "1                   0.0             0.0  southern_us  \n",
       "2                   0.0             0.0     filipino  \n",
       "3                   0.0             0.0       indian  \n",
       "4                   0.0             0.0       indian  \n",
       "...                 ...             ...          ...  \n",
       "39769               0.0             0.0        irish  \n",
       "39770               0.0             0.0      italian  \n",
       "39771               0.0             0.0        irish  \n",
       "39772               0.0             0.0      chinese  \n",
       "39773               0.0             0.0      mexican  \n",
       "\n",
       "[39774 rows x 3336 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>enokitake</th>\n      <th>frozen corn kernels</th>\n      <th>part-skim mozzarella</th>\n      <th>soda</th>\n      <th>california chile</th>\n      <th>condensed cream of celery soup</th>\n      <th>white bread flour</th>\n      <th>chile paste</th>\n      <th>basil</th>\n      <th>extra large shrimp</th>\n      <th>...</th>\n      <th>ale</th>\n      <th>fried eggs</th>\n      <th>asparagus tips</th>\n      <th>sofrito</th>\n      <th>calvados</th>\n      <th>field peas</th>\n      <th>fenugreek seeds</th>\n      <th>short-grain rice</th>\n      <th>glace cherries</th>\n      <th>cuisine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>greek</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>southern_us</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>filipino</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>indian</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>indian</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39769</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>irish</td>\n    </tr>\n    <tr>\n      <th>39770</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>italian</td>\n    </tr>\n    <tr>\n      <th>39771</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>irish</td>\n    </tr>\n    <tr>\n      <th>39772</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>chinese</td>\n    </tr>\n    <tr>\n      <th>39773</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>mexican</td>\n    </tr>\n  </tbody>\n</table>\n<p>39774 rows × 3336 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df[worthy_cols.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(dd[worthy_cols.columns].drop('cuisine', axis=1))"
   ]
  },
  {
   "source": [
    "Can cross-correlation help eliminate \"noise\"? (reduce overfitting)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                          enokitake  vegetable bouillon cube  \\\n",
       "enokitake                       NaN                      NaN   \n",
       "vegetable bouillon cube         NaN                      NaN   \n",
       "poppy seed dressing             NaN                      NaN   \n",
       "frozen corn kernels             NaN                      NaN   \n",
       "part-skim mozzarella            NaN                      NaN   \n",
       "...                             ...                      ...   \n",
       "extra lean minced beef          NaN                      NaN   \n",
       "watermelon radishes             NaN                      NaN   \n",
       "goat s milk cheese              NaN                      NaN   \n",
       "chilean sea bass fillets        NaN                      NaN   \n",
       "glace cherries                  NaN                      NaN   \n",
       "\n",
       "                          poppy seed dressing  frozen corn kernels  \\\n",
       "enokitake                                 NaN                  NaN   \n",
       "vegetable bouillon cube                   NaN                  NaN   \n",
       "poppy seed dressing                       NaN                  NaN   \n",
       "frozen corn kernels                       NaN                  NaN   \n",
       "part-skim mozzarella                      NaN                  NaN   \n",
       "...                                       ...                  ...   \n",
       "extra lean minced beef                    NaN                  NaN   \n",
       "watermelon radishes                       NaN             0.980617   \n",
       "goat s milk cheese                        NaN                  NaN   \n",
       "chilean sea bass fillets                  NaN                  NaN   \n",
       "glace cherries                            NaN                  NaN   \n",
       "\n",
       "                          part-skim mozzarella  soda  \\\n",
       "enokitake                                  NaN   NaN   \n",
       "vegetable bouillon cube                    NaN   NaN   \n",
       "poppy seed dressing                        NaN   NaN   \n",
       "frozen corn kernels                        NaN   NaN   \n",
       "part-skim mozzarella                       NaN   NaN   \n",
       "...                                        ...   ...   \n",
       "extra lean minced beef                0.995044   NaN   \n",
       "watermelon radishes                        NaN   NaN   \n",
       "goat s milk cheese                    0.995044   NaN   \n",
       "chilean sea bass fillets                   NaN   NaN   \n",
       "glace cherries                             NaN   NaN   \n",
       "\n",
       "                          skinless mahi mahi fillets  shortcakes  \\\n",
       "enokitake                                        NaN         NaN   \n",
       "vegetable bouillon cube                          NaN         NaN   \n",
       "poppy seed dressing                              NaN         NaN   \n",
       "frozen corn kernels                              NaN         NaN   \n",
       "part-skim mozzarella                             NaN         NaN   \n",
       "...                                              ...         ...   \n",
       "extra lean minced beef                           NaN         NaN   \n",
       "watermelon radishes                              NaN         NaN   \n",
       "goat s milk cheese                               NaN         NaN   \n",
       "chilean sea bass fillets                         NaN         NaN   \n",
       "glace cherries                                   NaN         NaN   \n",
       "\n",
       "                          california chile  bee pollen  ...  goma  field peas  \\\n",
       "enokitake                              NaN         NaN  ...   NaN         NaN   \n",
       "vegetable bouillon cube                NaN         NaN  ...   NaN         NaN   \n",
       "poppy seed dressing                    NaN         NaN  ...   NaN    0.992198   \n",
       "frozen corn kernels               0.920548         NaN  ...   NaN         NaN   \n",
       "part-skim mozzarella                   NaN         NaN  ...   NaN         NaN   \n",
       "...                                    ...         ...  ...   ...         ...   \n",
       "extra lean minced beef                 NaN         NaN  ...   NaN         NaN   \n",
       "watermelon radishes               0.947114         NaN  ...   NaN         NaN   \n",
       "goat s milk cheese                     NaN         NaN  ...   NaN         NaN   \n",
       "chilean sea bass fillets               NaN         NaN  ...   NaN         NaN   \n",
       "glace cherries                         NaN         NaN  ...   NaN         NaN   \n",
       "\n",
       "                          fenugreek seeds  pink food coloring  \\\n",
       "enokitake                             NaN                 NaN   \n",
       "vegetable bouillon cube               NaN                 NaN   \n",
       "poppy seed dressing                   NaN                 NaN   \n",
       "frozen corn kernels                   NaN                 NaN   \n",
       "part-skim mozzarella                  NaN                 NaN   \n",
       "...                                   ...                 ...   \n",
       "extra lean minced beef                NaN                 NaN   \n",
       "watermelon radishes                   NaN                 NaN   \n",
       "goat s milk cheese                    NaN                 NaN   \n",
       "chilean sea bass fillets              NaN                 NaN   \n",
       "glace cherries                        NaN                 NaN   \n",
       "\n",
       "                          short-grain rice  extra lean minced beef  \\\n",
       "enokitake                         0.907699                     NaN   \n",
       "vegetable bouillon cube                NaN                     NaN   \n",
       "poppy seed dressing                    NaN                     NaN   \n",
       "frozen corn kernels                    NaN                     NaN   \n",
       "part-skim mozzarella                   NaN                0.995044   \n",
       "...                                    ...                     ...   \n",
       "extra lean minced beef                 NaN                     NaN   \n",
       "watermelon radishes                    NaN                     NaN   \n",
       "goat s milk cheese                     NaN                1.000000   \n",
       "chilean sea bass fillets               NaN                     NaN   \n",
       "glace cherries                         NaN                     NaN   \n",
       "\n",
       "                          watermelon radishes  goat s milk cheese  \\\n",
       "enokitake                                 NaN                 NaN   \n",
       "vegetable bouillon cube                   NaN                 NaN   \n",
       "poppy seed dressing                       NaN                 NaN   \n",
       "frozen corn kernels                  0.980617                 NaN   \n",
       "part-skim mozzarella                      NaN            0.995044   \n",
       "...                                       ...                 ...   \n",
       "extra lean minced beef                    NaN            1.000000   \n",
       "watermelon radishes                       NaN                 NaN   \n",
       "goat s milk cheese                        NaN                 NaN   \n",
       "chilean sea bass fillets                  NaN                 NaN   \n",
       "glace cherries                            NaN                 NaN   \n",
       "\n",
       "                          chilean sea bass fillets  glace cherries  \n",
       "enokitake                                      NaN             NaN  \n",
       "vegetable bouillon cube                        NaN             NaN  \n",
       "poppy seed dressing                            NaN             NaN  \n",
       "frozen corn kernels                            NaN             NaN  \n",
       "part-skim mozzarella                           NaN             NaN  \n",
       "...                                            ...             ...  \n",
       "extra lean minced beef                         NaN             NaN  \n",
       "watermelon radishes                            NaN             NaN  \n",
       "goat s milk cheese                             NaN             NaN  \n",
       "chilean sea bass fillets                       NaN             NaN  \n",
       "glace cherries                                 NaN             NaN  \n",
       "\n",
       "[6714 rows x 6714 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>enokitake</th>\n      <th>vegetable bouillon cube</th>\n      <th>poppy seed dressing</th>\n      <th>frozen corn kernels</th>\n      <th>part-skim mozzarella</th>\n      <th>soda</th>\n      <th>skinless mahi mahi fillets</th>\n      <th>shortcakes</th>\n      <th>california chile</th>\n      <th>bee pollen</th>\n      <th>...</th>\n      <th>goma</th>\n      <th>field peas</th>\n      <th>fenugreek seeds</th>\n      <th>pink food coloring</th>\n      <th>short-grain rice</th>\n      <th>extra lean minced beef</th>\n      <th>watermelon radishes</th>\n      <th>goat s milk cheese</th>\n      <th>chilean sea bass fillets</th>\n      <th>glace cherries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>enokitake</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.907699</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>vegetable bouillon cube</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>poppy seed dressing</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.992198</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>frozen corn kernels</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.920548</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.980617</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>part-skim mozzarella</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.995044</td>\n      <td>NaN</td>\n      <td>0.995044</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>extra lean minced beef</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.995044</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>watermelon radishes</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.980617</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.947114</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>goat s milk cheese</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.995044</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>chilean sea bass fillets</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>glace cherries</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>6714 rows × 6714 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "corr = dd[col_sums.index].corr()\n",
    "for c in col_sums.index:\n",
    "    corr.loc[c, c] = 0\n",
    "corr[corr > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorr_cols = set(col_sums.index)\n",
    "for c in col_sums.index:\n",
    "    excess = corr[c] > 0.9\n",
    "    for c in excess.index:\n",
    "        if excess[c] and c in uncorr_cols:\n",
    "            uncorr_cols.remove(c)\n",
    "uncorr_cols.add('cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f80cd887e0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muncorr_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# oops, not a good idea after all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2cc59ec2fcbd>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(learn, test)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#gnb = CategoricalNB()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "learn(df[uncorr_cols]) # oops, not a good idea after all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}