# Part 1 - Puzzles

## Random Numbers
### Generating Random Numbers

You can see this as a real puzzle: write a program that produces a number randomly
chosen from a given interval - say, between 0 and 1. But this is impossible.

Some languages have a function whose output is an unpredictable number in a given
range. Say, if your computer uses LSE, you can type
```
?ALE(0)
```
and get an unpredictable number between 0 and 1, which you can consider random.

Other languages have built-in functions that do something like this. In those languages
producing a random number is no puzzle, it is trivial.

But if the language does not offer such a function, it is more than a puzzle - it is
simply impossible. Suppose we wrote a program like that. This program can have no
inputs, otherwise it is not the program that produces random numbers, but you input
something random... However, if it doesn't have any inputs, then it can only use
constants. But then there are no variables and repeated executions of the program
produce identical results. How could you get random numbers using such a program? [^1]

[^1]: Strictly speaking this is true of any program in any language, unless this program
   is using some external information as input. It is convenient to use something related
   to time as such input, like the number of changes of voltage in the power supply since
   turning on the computer, or the number of seconds since the computer manufacture, if
   your computer has an internal clock with an independent energy source (like its own
   lithium-ion battery). Usually, whatever the language you use, you have a way to access
   such a clock - see your documentation

So if your computer does not offer a way to get a random number, I can see only one
way to get it: enter a random number yourself. How can you do that? Well, how about this.
You can take a deck of 52 cards, shuffle it, split the deck into two parts, and take
three top cards from the bottom part. A small and very simple program can read three
integer numbers `x`, `y`, `z`. Use the card values as such numbers - say, Ace is 1, Jack
is 11, Queen is 12, King is 13. The computer works out:
```
(((x - 1) / 13 + y - 1) / 13 + z - 1) / 13
```
Fo example, you get a 7 of diamonds, a 10 of hearts and a 6 of diamons, then:
```
x = 7,   y = 10,   z = 6
```
and the computer produces 0.44060081929904416.

This number isn't truly unpredictable. As soon as you get the cards you can already tell
the order of magnitude of the result. On the other hand this way you cannot get more than
13<sup>3</sup>=2197 different numbers. But in practice this will suffice for the
applications we will consider in this book. And if you want to get something really
unpredictable, read the next section.

### Unpredictable Number Series

You rarely will need a single random number. Oftentimes you need to get a lot of such numbers.
Most of the games presented in this book require that the player encountered unpredictable
situations, allowed by the rules of the game. So we need to be able to generate such
situations.

Therefore we need the ability to obtain a chain of numbers that a transition from one
number to the next was determined using some simple computation, but at the same time it
would be difficult for the player to predict the result.

It is possible that the language that you use already has a function for this. For example,
in LSE there is a function `ALE(x)` which produces a number from the range (0,1), whose
value depends on `x`, but in an unpredictable way, which is, besides, not specified in the
language: the value wil  be different on different computers. If you execute the same
invocation three times:
```
ALE(0.1)
```
you are going to get the same return value, but there is no relationship between `ALE(0.1)`
and `ALE(0.2)` that would be easy to spot.

It may be interesting to have a look how it is possible to build such series of numbers.
Here is a method suggested by A.Engel [^ENG]. If you have a number `x` between 0 and 1,
then the next value in the series is:
```
fractional_part((x + 3.14159) ** 8)
```
Of course, you can compute the exponentiation to the power of 8 as squaring three times!
This produces a number between 9488 (for x = 0) and 86564. A very small change in `x` causes
a large change in (`x` + &pi;)<sup>8</sup>, and in particular it can cross the nearest
whole number, so the new result (the fractional part of the result) can turn out to be smaller
than the previous result.

Let's choose `x` = 0.52000. Then
```
(x + 3.14159) ** 8 = 32311.5436677282377786
```
so 0.52000 is followed by 0.5436677282377786.

But for `x` = 0.52005 you get 0.07361925091754529.

Since we get the fractional part, the resulting number is between 0 and 1.

**Exercise 1. The behaviour of the series.**
Let's explore how the number series produced this way behave. For this purpose we need to
compute a large number of elements of the series produced by the first element. Let's
place each element produced like this in one of 50 intervals of length 0.02, which together
cover the entire interval between 0 and 1. Let's output the number of elements in the series
that belong to each of the intervals. If the numbers of the series are uniformly
distributed in the interval (0,1), we should see that their numbers in each of the intervals
visibly tends to be constant.

Write a program that can verify this claim. Suppose, you can enter the starting number before
each computation.

**Exercise 2. Looking for other sequences.**
The use of &pi; in this series is arbitrary, so we could ask ourselves if the choice of the
number was the best. The numbers (`x` + 3.14159)<sup>8</sup> are fairly large, but we are using
only the fractional part. We discard the significant numbers of the whole part, and, because
the computer has only a fixed number of bits for the entire number, the fractional part uses
a smaller number of bits. Suppose the numbers are represented as 24 bits [^2]. You need 14 bits to
represent 9488 (note: this is the smallest number), because:
2<sup>13</sup> = 8192 < 9488 < 16384 = 2<sup>14</sup>
and 17 bits to record 86564 (note: this is the largest number), so you end up having only
between 7 and 10 bits for the fractional part.

[^2]: 24 bits was the size of fractional numbers the computers could typically work with at the
   time the original book has been written. The modern computers can manipulate with more bits
   to represent the fractional numbers, but the general argument remains valid: some bits of the
   fractional number are not used, when it is computed using this algorithm.

If you use (`x` + `a`)<sup>8</sup> instead of (`x` + &pi;)<sup>8</sup> for a value of `a` smaller
than &pi;, one may expect that more bits would be preserved for the fractional part. But you
cannot choose too small a value of `a` as in that case the distribution of numbers in the range
(0,1) will not be so good. Can you explain why?

For example, why can't you choose `a` = <sup>8</sup>&radic;2 ?

If you have done **Exercise 1** you have a program that can verify the series of random numbers.
Modify it so that you can input:
   - a constant `a`
   - the initial value of the sequence

Using my microcomputer I figured out that `a` = 1.226 produces much better results. But this can
vary from the machine to the machine, since this is very sensitive to the way the multiplication
is performed by the computer: the last bit of the multiplication is somewhat indeterminate, which
substantially affects this process.
